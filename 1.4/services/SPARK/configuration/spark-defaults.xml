<?xml version="1.0" encoding="UTF-8"?>
<!--
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
-->

<configuration supports_final="true">

  <property>
    <name>spark.yarn.executor.memoryOverhead</name>
    <value>384</value>
    <description>
      The amount of off heap memory (in megabytes) to be allocated per executor.
      This is memory that accounts for things like VM overheads, interned strings,
      other native overheads, etc.
    </description>
  </property>

  <property>
    <name>spark.yarn.driver.memoryOverhead</name>
    <value>384</value>
    <description>
      The amount of off heap memory (in megabytes) to be allocated per driver.
      This is memory that accounts for things like VM overheads, interned strings,
      other native overheads, etc.
    </description>
  </property>

  <property>
    <name>spark.yarn.scheduler.heartbeat.interval-ms</name>
    <value>5000</value>
    <description>
      The interval in ms in which the Spark application master heartbeats into the YARN ResourceManager.
    </description>
  </property>

  <property>
    <name>spark.yarn.max.executor.failures</name>
    <value>3</value>
    <description>
      The maximum number of executor failures before failing the application.
    </description>
  </property>

  <property>
    <name>spark.yarn.queue</name>
    <value>default</value>
    <description>
      The name of the YARN queue to which the application is submitted.
    </description>
  </property>

  <property>
    <name>spark.yarn.containerLauncherMaxThreads</name>
    <value>25</value>
    <description>
      The maximum number of threads to use in the application master for launching executor containers.
    </description>
  </property>

  <property>
    <name>spark.yarn.submit.file.replication</name>
    <value>3</value>
    <description>
      HDFS replication level for the files uploaded into HDFS for the application.
      These include things like the Spark jar, the app jar, and any distributed cache files/archives.
    </description>
  </property>

  <property>
    <name>spark.yarn.preserve.staging.files</name>
    <value>false</value>
    <description>
      Set to true to preserve the staged files (Spark jar, app jar, distributed cache files) at the
      end of the job rather then delete them.
    </description>
  </property>

  <property>
    <name>spark.yarn.services</name>
    <value>org.apache.spark.deploy.history.FsHistoryProvider</value>
    <description>
      Service required for publishing events to the YARN Application Timeline Service.
    </description>
  </property>

  <property>
    <name>spark.history.provider</name>
    <value>org.apache.spark.deploy.history.FsHistoryProvider</value>
    <description>
      Name of the class implementing the application history backend which publishes to YARN Application Timeline Service.
    </description>
  </property>

  <property>
    <name>spark.eventLog.dir</name>
    <value>/var/log/spark/</value>
    <description>Base directory in which Spark events are logged, if spark.eventLog.enabled is true. Within this base directory, Spark creates a sub-directory for each application, and logs the events specific to the application in this directory. Users may want to set this to a unified location like an HDFS directory so history files can be read by the history server.</description>
  </property>
 <property>
    <name>spark.history.fs.logDirectory</name>
    <value>file:/tmp/</value>
<description>
For the filesystem history provider, the URL to the directory containing application event logs to load. This can be a local file:// path, an HDFS path hdfs://namenode/shared/spark-logs or that of an alternative filesystem supported by the Hadoop APIs
</description>
  </property>

  <property>
    <name>spark.history.ui.port</name>
    <value>18085</value>
    <description>
      The port to which the web interface of the History Server binds.
    </description>
  </property>

  <property>
    <name>spark.history.kerberos.principal</name>
    <value>none</value>
    <description>
      Kerberos principal name for the Spark History Server.
    </description>
  </property>

  <property>
    <name>spark.history.kerberos.keytab</name>
    <value>none</value>
    <description>
      Location of the kerberos keytab file for the Spark History Server.
    </description>
  </property>

  <property>
    <name>spark.yarn.historyServer.address</name>
    <value>localhost:18085</value>
    <description>The address of the Spark history server (i.e. host.com:18080). The address should not contain a scheme (http://). Defaults to not being set since the history server is an optional service. This address is given to the YARN ResourceManager when the Spark application finishes to link the application from the ResourceManager UI to the Spark history server UI.</description>
  </property>

  <property>
    <name>spark.yarn.max.executor.failures</name>
    <value>3</value>
    <description>The maximum number of executor failures before failing the application.</description>
  </property>

</configuration>
